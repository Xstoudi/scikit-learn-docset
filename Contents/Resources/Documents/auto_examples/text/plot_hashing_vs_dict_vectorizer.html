
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="Docutils 0.19: https://docutils.sourceforge.io/" name="generator">
<meta content="FeatureHasher and DictVectorizer Comparison" property="og:title"/>
<meta content="website" property="og:type"/>
<meta content="https://scikit-learn/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html" property="og:url"/>
<meta content="scikit-learn" property="og:site_name"/>
<meta content="In this example we illustrate text vectorization, which is the process of representing non-numerical input data (such as dictionaries or text documents) as vectors of real numbers. We first compare..." property="og:description"/>
<meta content="https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png" property="og:image"/>
<meta content="scikit-learn" property="og:image:alt"/>
<meta content="In this example we illustrate text vectorization, which is the process of representing non-numerical input data (such as dictionaries or text documents) as vectors of real numbers. We first compare..." name="description"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>FeatureHasher and DictVectorizer Comparison — scikit-learn 1.2.2 documentation</title>
<link href="http://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html" rel="canonical"/>
<link href="../../_static/favicon.ico" rel="shortcut icon"/>
<link href="../../_static/css/vendor/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/plot_directive.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-binder.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-dataframe.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-rendered-html.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
<script src="../../_static/js/vendor/jquery-3.6.3.slim.min.js"></script>
</meta></head>
<body>
<nav class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0" id="navbar">
<div class="container-fluid sk-docs-container px-0">
<a class="navbar-brand py-0" href="../../index.html">
<img alt="logo" class="sk-brand-img" src="../../_static/scikit-learn-logo-small.png"/>
</a>
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#navbarSupportedContent" data-toggle="collapse" id="sk-navbar-toggler" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
<ul class="navbar-nav mr-auto">
<li class="nav-item">
<a class="sk-nav-link nav-link" href="../../install.html">Install</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link" href="../../user_guide.html">User Guide</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link" href="../../modules/classes.html">API</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link" href="../index.html">Examples</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link" href="https://blog.scikit-learn.org/" rel="noopener noreferrer" target="_blank">Community</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../getting_started.html">Getting Started</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../tutorial/index.html">Tutorial</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../whats_new/v1.2.html">What's new</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../glossary.html">Glossary</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/developers/index.html" rel="noopener noreferrer" target="_blank">Development</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../faq.html">FAQ</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../support.html">Support</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../related_projects.html">Related packages</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../roadmap.html">Roadmap</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../governance.html">Governance</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../about.html">About us</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
</li>
<li class="nav-item">
<a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Other Versions and Download</a>
</li>
<li class="nav-item dropdown nav-more-item-dropdown">
<a aria-expanded="false" aria-haspopup="true" class="sk-nav-link nav-link dropdown-toggle" data-toggle="dropdown" href="#" id="navbarDropdown" role="button">More</a>
<div aria-labelledby="navbarDropdown" class="dropdown-menu">
<a class="sk-nav-dropdown-item dropdown-item" href="../../getting_started.html">Getting Started</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../tutorial/index.html">Tutorial</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../whats_new/v1.2.html">What's new</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../glossary.html">Glossary</a>
<a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/developers/index.html" rel="noopener noreferrer" target="_blank">Development</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../faq.html">FAQ</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../support.html">Support</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../related_projects.html">Related packages</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../roadmap.html">Roadmap</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../governance.html">Governance</a>
<a class="sk-nav-dropdown-item dropdown-item" href="../../about.html">About us</a>
<a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
<a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Other Versions and Download</a>
</div>
</li>
</ul>
<div id="searchbox" role="search">
<div class="searchformwrapper">
<form action="../../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" class="sk-search-text-input" name="q" type="text"/>
<input class="sk-search-text-btn" type="submit" value="Go"/>
</form>
</div>
</div>
</div>
</div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
<input id="sk-toggle-checkbox" name="sk-toggle-checkbox" type="checkbox"/>
<label class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox" id="sk-sidemenu-toggle">Toggle Menu</label>
<div class="border-right" id="sk-sidebar-wrapper">
<div class="sk-sidebar-toc-wrapper">
<div aria-label="rellinks" class="btn-group w-100 mb-2" role="group">
<a class="btn sk-btn-rellink py-1" href="plot_document_clustering.html" role="button" sk-rellink-tooltip="Clustering text documents using k-means">Prev</a><a class="btn sk-btn-rellink py-1" href="index.html" role="button" sk-rellink-tooltip="Working with text documents">Up</a>
<a class="btn sk-btn-rellink py-1" href="../../modules/classes.html" role="button" sk-rellink-tooltip="API Reference">Next</a>
</div>
<div class="alert alert-danger p-1 mb-2" role="alert">
<p class="text-center mb-0">
<strong>scikit-learn 1.2.2</strong><br/>
<a href="http://scikit-learn.org/dev/versions.html">Other versions</a>
</p>
</div>
<div class="alert alert-warning p-1 mb-2" role="alert">
<p class="text-center mb-0">
            Please <a class="font-weight-bold" href="../../about.html#citing-scikit-learn"><string>cite us</string></a> if you use the software.
          </p>
</div>
<div class="sk-sidebar-toc">
<ul>
<li><a class="reference internal" href="#">FeatureHasher and DictVectorizer Comparison</a><ul>
<li><a class="reference internal" href="#load-data">Load Data</a></li>
<li><a class="reference internal" href="#define-preprocessing-functions">Define preprocessing functions</a></li>
<li><a class="reference internal" href="#dictvectorizer">DictVectorizer</a></li>
<li><a class="reference internal" href="#featurehasher">FeatureHasher</a></li>
<li><a class="reference internal" href="#comparison-with-special-purpose-text-vectorizers">Comparison with special purpose text vectorizers</a></li>
<li><a class="reference internal" href="#tfidfvectorizer">TfidfVectorizer</a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div id="sk-page-content-wrapper">
<div class="sk-page-content container-fluid body px-md-3" role="main">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">here</span></a>
to download the full example code or to run this example in your browser via Binder</p>
</div>
<section class="sphx-glr-example-title" id="featurehasher-and-dictvectorizer-comparison">
<a class="dashAnchor" name="//apple_ref/cpp/Section/FeatureHasher and DictVectorizer Comparison"></a><span id="sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"></span><h1>FeatureHasher and DictVectorizer Comparison<a class="headerlink" href="#featurehasher-and-dictvectorizer-comparison" title="Permalink to this heading">¶</a></h1>
<p>In this example we illustrate text vectorization, which is the process of
representing non-numerical input data (such as dictionaries or text documents)
as vectors of real numbers.</p>
<p>We first compare <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> and
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> by using both methods to
vectorize text documents that are preprocessed (tokenized) with the help of a
custom Python function.</p>
<p>Later we introduce and analyze the text-specific vectorizers
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a>,
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> and
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a> that handle both the
tokenization and the assembling of the feature matrix within a single class.</p>
<p>The objective of the example is to demonstrate the usage of text vectorization
API and to compare their processing time. See the example scripts
<a class="reference internal" href="plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>
and <a class="reference internal" href="plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a> for actual
learning on text documents.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Lars Buitinck</span>
<span class="c1">#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="c1">#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr&gt;</span>
<span class="c1"># License: BSD 3 clause</span>
</pre></div>
</div>
<section id="load-data">
<h2>Load Data<a class="headerlink" href="#load-data" title="Permalink to this heading">¶</a></h2>
<p>We load data from <a class="reference internal" href="../../datasets/real_world.html#newsgroups-dataset"><span class="std std-ref">The 20 newsgroups text dataset</span></a>, which comprises around
18000 newsgroups posts on 20 topics split in two subsets: one for training and
one for testing. For the sake of simplicity and reducing the computational
cost, we select a subset of 7 topics and use the training set only.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function" href="../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><span class="n">fetch_20newsgroups</span></a>

<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"alt.atheism"</span><span class="p">,</span>
    <span class="s2">"comp.graphics"</span><span class="p">,</span>
    <span class="s2">"comp.sys.ibm.pc.hardware"</span><span class="p">,</span>
    <span class="s2">"misc.forsale"</span><span class="p">,</span>
    <span class="s2">"rec.autos"</span><span class="p">,</span>
    <span class="s2">"sci.space"</span><span class="p">,</span>
    <span class="s2">"talk.religion.misc"</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Loading 20 newsgroups training data"</span><span class="p">)</span>
<span class="n">raw_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-datasets sphx-glr-backref-type-py-function" href="../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><span class="n">fetch_20newsgroups</span></a><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s2">"train"</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">data_size_mb</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e6</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents - </span><span class="si">{</span><span class="n">data_size_mb</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">MB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Loading 20 newsgroups training data
3803 documents - 6.245MB
</pre></div>
</div>
</section>
<section id="define-preprocessing-functions">
<h2>Define preprocessing functions<a class="headerlink" href="#define-preprocessing-functions" title="Permalink to this heading">¶</a></h2>
<p>A token may be a word, part of a word or anything comprised between spaces or
symbols in a string. Here we define a function that extracts the tokens using
a simple regular expression (regex) that matches Unicode word characters. This
includes most characters that can be part of a word in any language, as well
as numbers and the underscore:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>


<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Extract tokens from doc.</span>

<span class="sd">    This uses a simple regex that matches word characters to break strings</span>
<span class="sd">    into tokens. For a more principled approach, see CountVectorizer or</span>
<span class="sd">    TfidfVectorizer.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tok</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <a class="sphx-glr-backref-module-re sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/re.html#re.findall" title="re.findall"><span class="n">re</span><span class="o">.</span><span class="n">findall</span></a><span class="p">(</span><span class="sa">r</span><span class="s2">"\w+"</span><span class="p">,</span> <span class="n">doc</span><span class="p">))</span>


<span class="nb">list</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"This is a simple example, isn't it?"</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>['this', 'is', 'a', 'simple', 'example', 'isn', 't', 'it']
</pre></div>
</div>
<p>We define an additional function that counts the (frequency of) occurrence of
each token in a given document. It returns a frequency dictionary to be used
by the vectorizers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-collections sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/collections.html#collections.defaultdict" title="collections.defaultdict"><span class="n">defaultdict</span></a>


<span class="k">def</span> <span class="nf">token_freqs</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Extract a dict mapping tokens from doc to their occurrences."""</span>

    <span class="n">freq</span> <span class="o">=</span> <a class="sphx-glr-backref-module-collections sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/collections.html#collections.defaultdict" title="collections.defaultdict"><span class="n">defaultdict</span></a><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
        <span class="n">freq</span><span class="p">[</span><span class="n">tok</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">freq</span>


<span class="n">token_freqs</span><span class="p">(</span><span class="s2">"That is one example, but this is another one"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>defaultdict(&lt;class 'int'&gt;, {'that': 1, 'is': 2, 'one': 2, 'example': 1, 'but': 1, 'this': 1, 'another': 1})
</pre></div>
</div>
<p>Observe in particular that the repeated token <code class="docutils literal notranslate"><span class="pre">"is"</span></code> is counted twice for
instance.</p>
<p>Breaking a text document into word tokens, potentially losing the order
information between the words in a sentence is often called a <a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words
representation</a>.</p>
</section>
<section id="dictvectorizer">
<h2>DictVectorizer<a class="headerlink" href="#dictvectorizer" title="Permalink to this heading">¶</a></h2>
<p>First we benchmark the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a>,
then we compare it to <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> as
both of them receive dictionaries as input.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><span class="n">DictVectorizer</span></a>

<span class="n">dict_count_vectorizers</span> <span class="o">=</span> <a class="sphx-glr-backref-module-collections sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/collections.html#collections.defaultdict" title="collections.defaultdict"><span class="n">defaultdict</span></a><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">vectorizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><span class="n">DictVectorizer</span></a><span class="p">()</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">token_freqs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">vectorizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">on freq dicts"</span>
<span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span><span class="si">}</span><span class="s2"> unique terms"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 1.016 s at 6.1 MB/s
Found 47928 unique terms
</pre></div>
</div>
<p>The actual mapping from text token to column index is explicitly stored in
the <code class="docutils literal notranslate"><span class="pre">.vocabulary_</span></code> attribute which is a potentially very large Python
dictionary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>47928
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="s2">"example"</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>19145
</pre></div>
</div>
</section>
<section id="featurehasher">
<h2>FeatureHasher<a class="headerlink" href="#featurehasher" title="Permalink to this heading">¶</a></h2>
<p>Dictionaries take up a large amount of storage space and grow in size as the
training set grows. Instead of growing the vectors along with a dictionary,
feature hashing builds a vector of pre-defined length by applying a hash
function <code class="docutils literal notranslate"><span class="pre">h</span></code> to the features (e.g., tokens), then using the hash values
directly as feature indices and updating the resulting vector at those
indices. When the feature space is not large enough, hashing functions tend to
map distinct values to the same hash code (hash collisions). As a result, it
is impossible to determine what object generated any particular hash code.</p>
<p>Because of the above it is impossible to recover the original tokens from the
feature matrix and the best approach to estimate the number of unique terms in
the original dictionary is to count the number of active columns in the
encoded feature matrix. For such a purpose we define the following function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">n_nonzero_columns</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Number of columns with at least one non-zero value in a CSR matrix.</span>

<span class="sd">    This is useful to count the number of features columns that are effectively</span>
<span class="sd">    active when using the FeatureHasher.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function" href="https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy.unique" title="numpy.unique"><span class="n">np</span><span class="o">.</span><span class="n">unique</span></a><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>The default number of features for the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> is 2**20. Here we set
<code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">=</span> <span class="pre">2**18</span></code> to illustrate hash collisions.</p>
<p><strong>FeatureHasher on frequency dictionaries</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><span class="n">FeatureHasher</span></a>

<span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">hasher</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><span class="n">FeatureHasher</span></a><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">18</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">token_freqs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">hasher</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">on freq dicts"</span>
<span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="n">n_nonzero_columns</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2"> unique tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.544 s at 11.5 MB/s
Found 43873 unique tokens
</pre></div>
</div>
<p>The number of unique tokens when using the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> is lower than those obtained
using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a>. This is due to
hash collisions.</p>
<p>The number of collisions can be reduced by increasing the feature space.
Notice that the speed of the vectorizer does not change significantly when
setting a large number of features, though it causes larger coefficient
dimensions and then requires more memory usage to store them, even if a
majority of them is inactive.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">hasher</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><span class="n">FeatureHasher</span></a><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">22</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">token_freqs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="n">n_nonzero_columns</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2"> unique tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.545 s at 11.5 MB/s
Found 47668 unique tokens
</pre></div>
</div>
<p>We confirm that the number of unique tokens gets closer to the number of
unique terms found by the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a>.</p>
<p><strong>FeatureHasher on raw tokens</strong></p>
<p>Alternatively, one can set <code class="docutils literal notranslate"><span class="pre">input_type="string"</span></code> in the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> to vectorize the strings
output directly from the customized <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> function. This is equivalent to
passing a dictionary with an implied frequency of 1 for each feature name.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">hasher</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><span class="n">FeatureHasher</span></a><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">18</span><span class="p">,</span> <span class="n">input_type</span><span class="o">=</span><span class="s2">"string"</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">hasher</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">on raw tokens"</span>
<span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="n">n_nonzero_columns</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2"> unique tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.505 s at 12.4 MB/s
Found 43873 unique tokens
</pre></div>
</div>
<p>We now plot the speed of the above methods for vectorizing.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">y_pos</span> <span class="o">=</span> <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"speed (MB/s)"</span><span class="p">)</span>
</pre></div>
</div>
<img alt="plot hashing vs dict vectorizer" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_001.png" srcset="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_001.png"/><p>In both cases <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> is
approximately twice as fast as
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a>. This is handy when dealing
with large amounts of data, with the downside of losing the invertibility of
the transformation, which in turn makes the interpretation of a model a more
complex task.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">FeatureHeasher</span></code> with <code class="docutils literal notranslate"><span class="pre">input_type="string"</span></code> is slightly faster than the
variant that works on frequency dict because it does not count repeated
tokens: each token is implicitly counted once, even if it was repeated.
Depending on the downstream machine learning task, it can be a limitation or
not.</p>
</section>
<section id="comparison-with-special-purpose-text-vectorizers">
<h2>Comparison with special purpose text vectorizers<a class="headerlink" href="#comparison-with-special-purpose-text-vectorizers" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> accepts raw data as
it internally implements tokenization and occurrence counting. It is similar
to the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> when used along with
the customized function <code class="docutils literal notranslate"><span class="pre">token_freqs</span></code> as done in the previous section. The
difference being that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>
is more flexible. In particular it accepts various regex patterns through the
<code class="docutils literal notranslate"><span class="pre">token_pattern</span></code> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><span class="n">CountVectorizer</span></a>

<span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">vectorizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><span class="n">CountVectorizer</span></a><span class="p">()</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span><span class="si">}</span><span class="s2"> unique terms"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.661 s at 9.5 MB/s
Found 47885 unique terms
</pre></div>
</div>
<p>We see that using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>
implementation is approximately twice as fast as using the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> along with the simple
function we defined for mapping the tokens. The reason is that
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> is optimized by
reusing a compiled regular expression for the full training set instead of
creating one per document as done in our naive tokenize function.</p>
<p>Now we make a similar experiment with the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a>, which is
equivalent to combining the “hashing trick” implemented by the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> class and the text
preprocessing and tokenization of the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><span class="n">HashingVectorizer</span></a>

<span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">vectorizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><span class="n">HashingVectorizer</span></a><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">18</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.467 s at 13.4 MB/s
</pre></div>
</div>
<p>We can observe that this is the fastest text tokenization strategy so far,
assuming that the downstream machine learning task can tolerate a few
collisions.</p>
</section>
<section id="tfidfvectorizer">
<h2>TfidfVectorizer<a class="headerlink" href="#tfidfvectorizer" title="Permalink to this heading">¶</a></h2>
<p>In a large text corpus, some words appear with higher frequency (e.g. “the”,
“a”, “is” in English) and do not carry meaningful information about the actual
contents of a document. If we were to feed the word count data directly to a
classifier, those very common terms would shadow the frequencies of rarer yet
more informative terms. In order to re-weight the count features into floating
point values suitable for usage by a classifier it is very common to use the
tf–idf transform as implemented by the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a>. TF stands for
“term-frequency” while “tf–idf” means term-frequency times inverse
document-frequency.</p>
<p>We now benchmark the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a>,
which is equivalent to combining the tokenization and occurrence counting of
the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> along with the
normalizing and weighting from a
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><span class="n">TfidfVectorizer</span></a>

<span class="n">t0</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span>
<span class="n">vectorizer</span> <span class="o">=</span> <a class="sphx-glr-backref-module-sklearn-feature_extraction-text sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><span class="n">TfidfVectorizer</span></a><span class="p">()</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <a class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function" href="https://docs.python.org/3/library/time.html#time.time" title="time.time"><span class="n">time</span></a><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_size_mb</span> <span class="o">/</span> <span class="n">duration</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"done in </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> s at </span><span class="si">{</span><span class="n">data_size_mb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">duration</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB/s"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span><span class="si">}</span><span class="s2"> unique terms"</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>done in 0.669 s at 9.3 MB/s
Found 47885 unique terms
</pre></div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<p>Let’s conclude this notebook by summarizing all the recorded processing speeds
in a single plot:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">y_pos</span> <span class="o">=</span> <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"speed"</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">dict_count_vectorizers</span><span class="p">[</span><span class="s2">"vectorizer"</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"speed (MB/s)"</span><span class="p">)</span>
</pre></div>
</div>
<img alt="plot hashing vs dict vectorizer" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_002.png" srcset="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_002.png"/><p>Notice from the plot that
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a> is slightly slower
than <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> because of the
extra operation induced by the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-func docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a>.</p>
<p>Also notice that, by setting the number of features <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">=</span> <span class="pre">2**18</span></code>, the
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a> performs better
than the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> at the
expense of inversibility of the transformation due to hash collisions.</p>
<p>We highlight that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> and
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a> perform better than
their equivalent <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-func docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> and
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-func docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> on manually tokenized
documents since the internal tokenization step of the former vectorizers
compiles a regular expression once and then reuses it for all the documents.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  4.892 seconds)</p>
<a class="dashAnchor" name="//apple_ref/cpp/Section/sphx_glr_download_auto_examples_text_plot_hashing_vs_dict_vectorizer.py"></a><div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-text-plot-hashing-vs-dict-vectorizer-py">
<div class="binder-badge docutils container">
<a class="reference external image-reference" href="https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.2.X?urlpath=lab/tree/notebooks/auto_examples/text/plot_hashing_vs_dict_vectorizer.ipynb"><img alt="Launch binder" src="../../_images/binder_badge_logo19.svg" width="150px"/></a>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/5775388ede077a05b00514ecbaa17f32/plot_hashing_vs_dict_vectorizer.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_hashing_vs_dict_vectorizer.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/06cfc926acb27652fb2aa5bfc583e7cb/plot_hashing_vs_dict_vectorizer.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_hashing_vs_dict_vectorizer.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</div>
<div class="container">
<footer class="sk-content-footer">
            © 2007 - 2023, scikit-learn developers (BSD License).
          <a href="../../_sources/auto_examples/text/plot_hashing_vs_dict_vectorizer.rst.txt" rel="nofollow">Show this page source</a>
</footer>
</div>
</div>
</div>
<script src="../../_static/js/vendor/bootstrap.min.js"></script>
<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script data-domain="scikit-learn.org" defer="" src="https://views.scientific-python.org/js/script.js">
</script>
<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
});

</script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>